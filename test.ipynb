{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Embedding,Dense,Dropout,Bidirectional,LSTM,GRU,Conv1D,Layer\n",
    "from tensorflow.keras.models import load_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_feature(seq_matrix):\n",
    "    \"\"\"将字符编码为整数\n",
    "    \"\"\"\n",
    "    seq_matrix = list(seq_matrix)\n",
    "    # print(seq_matrix)\n",
    "    ind_to_char = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M','N','P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X']\n",
    "    char_to_ind = {char: i for i, char in enumerate(ind_to_char)}\n",
    "    #return [ind_to_char.index(i) for i in list(seq_matrix)]\n",
    "    return [char_to_ind[i] for i in seq_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#定义SN、SP、ACC、MCC\n",
    "def sn_sp_acc_mcc(true_label,predict_label,pos_label=1):\n",
    "    import math\n",
    "    pos_num = np.sum(true_label==pos_label)\n",
    "    print('pos_num=',pos_num)\n",
    "    neg_num = true_label.shape[0]-pos_num\n",
    "    print('neg_num=',neg_num)\n",
    "    tp =np.sum((true_label==pos_label) & (predict_label==pos_label))\n",
    "    print('tp=',tp)\n",
    "    tn = np.sum(true_label==predict_label)-tp\n",
    "    print('tn=',tn)\n",
    "    sn = tp/pos_num\n",
    "    sp = tn/neg_num\n",
    "    acc = (tp+tn)/(pos_num+neg_num)\n",
    "    fn = pos_num - tp\n",
    "    fp = neg_num - tn\n",
    "    print('fn=',fn)\n",
    "    print('fp=',fp)\n",
    "    \n",
    "    tp = np.array(tp,dtype=np.float64)\n",
    "    tn = np.array(tn,dtype=np.float64)\n",
    "    fp = np.array(fp,dtype=np.float64)\n",
    "    fn = np.array(fn,dtype=np.float64)\n",
    "    mcc = (tp*tn-fp*fn)/(np.sqrt((tp+fn)*(tp+fp)*(tn+fp)*(tn+fn)))\n",
    "    return sn,sp,acc,mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Attention3d(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention3d, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"W_regularizer\": self.W_regularizer,\n",
    "                  \"b_regularizer\": self.b_regularizer, \"W_constraint\": self.W_constraint,\n",
    "                  \"b_constraint\": self.b_constraint,\n",
    "                  \"bias\": self.bias, \"step_dim\": self.step_dim, \"features_dim\": self.features_dim}\n",
    "        base_config = super(Attention3d, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=initializers.get('glorot_uniform'),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),\n",
    "                      (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    maxlen = 33\n",
    "    max_features = 21\n",
    "    embedding_dims = 64\n",
    "    class_num = 1\n",
    "    last_activation = 'sigmoid'\n",
    "    input = Input((maxlen,))\n",
    "    embedding = Embedding(max_features, embedding_dims, input_length=maxlen)(input)\n",
    "\n",
    "    x = Bidirectional(GRU(64, return_sequences=True))(embedding)\n",
    "    x = Bidirectional(GRU(32, return_sequences=True))(x)\n",
    "#     x = Bidirectional(GRU(16, return_sequences=True))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Attention3d(maxlen)(x)\n",
    "\n",
    "    t = Dense(16,activation='relu')(x)\n",
    "    output = Dense(class_num, activation=last_activation)(t)\n",
    "    model = Model(inputs=input, outputs=output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_predict(id_seqs):\n",
    "    id = []\n",
    "    seqs = []\n",
    "    site = []\n",
    "    is_phosphory = []\n",
    "    prob = []\n",
    "    seq_len = len(id_seqs)\n",
    "    for i in range(seq_len):\n",
    "        record = id_seqs[i]\n",
    "        if len(record) >= 33:\n",
    "            for j in range(len(record)-32):\n",
    "                seq = record[j:j+33]\n",
    "                seq = seq.upper()\n",
    "                for seq_one in seq:\n",
    "                    if seq_one in 'ACDEFGHIKLMNPQRSTVWYX':\n",
    "                        continue\n",
    "                    else:\n",
    "                        return \"Please enter the sequence of 20 amino acids or 'X'\"\n",
    "                if seq[16] in \"ST\":\n",
    "                    fea_df = get_feature(seq)\n",
    "                    feature = np.array(fea_df)\n",
    "                    feature = np.expand_dims(feature, axis=0)\n",
    "                    model = define_model()\n",
    "                    model.load_weights('models/ST_model.h5')\n",
    "                    res = model.predict(feature)\n",
    "                    id.append(id_seqs[i-1].split('>')[-1])\n",
    "                    seqs.append(seq)\n",
    "                    # print(1, seq[17])\n",
    "                    site.append(seq[16])\n",
    "                    prob.append(res)\n",
    "                    if res > 0.5:\n",
    "                        is_phosphory.append(\"True\")\n",
    "                    else:\n",
    "                        is_phosphory.append(\"False\")\n",
    "                elif seq[16] == \"Y\":\n",
    "                    # print(2, seq[17])\n",
    "                    fea_df = get_feature(seq)\n",
    "                    feature = np.array(fea_df)\n",
    "                    feature = np.expand_dims(feature, axis=0)\n",
    "                    model = define_model()\n",
    "                    model.load_weights('models/Y_model.h5')\n",
    "                    res = model.predict(feature)\n",
    "                    id.append(id_seqs[i - 1].split('>')[-1])\n",
    "                    seqs.append(seq)\n",
    "                    site.append(\"Y\")\n",
    "                    prob.append(res)\n",
    "                    if res > 0.5:\n",
    "                        is_phosphory.append(\"True\")\n",
    "                    else:\n",
    "                        is_phosphory.append(\"False\")\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            id.append(id_seqs[i - 1].split('>')[-1])\n",
    "            seqs.append(\"Sequence length must be >= 33\")\n",
    "            site.append(\"-\")\n",
    "            is_phosphory.append(\"-\")\n",
    "            prob.append(\"-\")\n",
    "    res_df = pd.DataFrame(columns=['id', 'seq', 'site', 'is_phosphory', 'prob'])\n",
    "    res_df.id = id\n",
    "    res_df.seq = seqs\n",
    "    res_df.site = site\n",
    "    res_df.is_phosphory = is_phosphory\n",
    "    res_df.prob = prob\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    data = \"fastafile\"\n",
    "    predict_result = run_predict(data)\n",
    "    print(predict_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}